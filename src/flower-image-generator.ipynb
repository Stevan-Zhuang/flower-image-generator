{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":null,"source":["import os\r\n","import argparse\r\n","\r\n","import torch\r\n","from torch import nn\r\n","from torch.nn import functional as F\r\n","from torch import optim\r\n","from torch.utils.data import DataLoader\r\n","from torchvision.datasets import ImageFolder\r\n","from torchvision import transforms as T\r\n","from torchvision.utils import make_grid\r\n","import pytorch_lightning as pl\r\n","from pytorch_lightning import seed_everything\r\n","\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","\r\n","from typing import Optional, List, Any"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["seed_everything(6)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["config = argparse.Namespace(\r\n","    gpu=True,\r\n","    data_dir=\"../input/pytorch-challange-flower-dataset/dataset\",\r\n","    n_epochs=40,\r\n","    batch_size=128,\r\n","    \r\n","    min_image_size=500,\r\n","    image_size=64,\r\n","    n_channels=3,\r\n",")"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["class ImageDataModule(pl.LightningDataModule):\r\n","    def __init__(self, data_dir: str, min_image_size: str,\r\n","                 image_size: Optional[int] = 64,\r\n","                 batch_size: Optional[int] = 1) -> None:\r\n","        super(ImageDataModule, self).__init__()\r\n","        \r\n","        self.data_dir = data_dir\r\n","        self.batch_size = batch_size\r\n","        self.image_size = image_size\r\n","\r\n","    def setup(self, stage: Optional[str] = None) -> None:\r\n","        pipeline = T.Compose([\r\n","            T.CenterCrop(500),\r\n","            T.Resize(self.image_size),\r\n","            T.ToTensor(),\r\n","            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r\n","        ])\r\n","        self.train_dataset = ImageFolder(\r\n","            self.data_dir, transform=pipeline\r\n","        )\r\n","\r\n","    def train_dataloader(self) -> DataLoader:\r\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size,\r\n","                          shuffle=True)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["def demo_single(model: torch.nn.Module) -> None:\r\n","    \"\"\"Create a demo of a single image generated by model.\"\"\"\r\n","    single = (model(torch.rand(1, 100)).squeeze(0) * 0.5 + 0.5).detach()\r\n","    plt.imshow(single.permute(1, 2, 0))\r\n","    plt.show()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["def demo_grid(model: torch.nn.Module) -> None:\r\n","    \"\"\"Create a 3 x 5 demo of images generated by model.\"\"\"\r\n","    grid = make_grid(model(torch.randn(15, 100)).detach(), nrow=5, normalize=True)\r\n","    plt.imshow(grid.permute(1, 2, 0))\r\n","    plt.show()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Copied from pytorch-lightning-bolts DCGAN\r\n","\r\n","class DCGANGenerator(nn.Module):\r\n","    def __init__(self, latent_dim: int, feature_maps: int, image_channels: int) -> None:\r\n","        \"\"\"\r\n","        Args:\r\n","            latent_dim: Dimension of the latent space\r\n","            feature_maps: Number of feature maps to use\r\n","            image_channels: Number of channels of the images from the dataset\r\n","        \"\"\"\r\n","        super().__init__()\r\n","        self.gen = nn.Sequential(\r\n","            self._make_gen_block(latent_dim, feature_maps * 8, kernel_size=4, stride=1, padding=0),\r\n","            self._make_gen_block(feature_maps * 8, feature_maps * 4),\r\n","            self._make_gen_block(feature_maps * 4, feature_maps * 2),\r\n","            self._make_gen_block(feature_maps * 2, feature_maps),\r\n","            self._make_gen_block(feature_maps, image_channels, last_block=True),\r\n","        )\r\n","\r\n","    @staticmethod\r\n","    def _make_gen_block(\r\n","        in_channels: int,\r\n","        out_channels: int,\r\n","        kernel_size: int = 4,\r\n","        stride: int = 2,\r\n","        padding: int = 1,\r\n","        bias: bool = False,\r\n","        last_block: bool = False,\r\n","    ) -> nn.Sequential:\r\n","        if not last_block:\r\n","            gen_block = nn.Sequential(\r\n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\r\n","                nn.BatchNorm2d(out_channels),\r\n","                nn.ReLU(True),\r\n","            )\r\n","        else:\r\n","            gen_block = nn.Sequential(\r\n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\r\n","                nn.Tanh(),\r\n","            )\r\n","\r\n","        return gen_block\r\n","\r\n","    def forward(self, noise: torch.Tensor) -> torch.Tensor:\r\n","        return self.gen(noise)\r\n","\r\n","\r\n","class DCGANDiscriminator(nn.Module):\r\n","    def __init__(self, feature_maps: int, image_channels: int) -> None:\r\n","        \"\"\"\r\n","        Args:\r\n","            feature_maps: Number of feature maps to use\r\n","            image_channels: Number of channels of the images from the dataset\r\n","        \"\"\"\r\n","        super().__init__()\r\n","        self.disc = nn.Sequential(\r\n","            self._make_disc_block(image_channels, feature_maps, batch_norm=False),\r\n","            self._make_disc_block(feature_maps, feature_maps * 2),\r\n","            self._make_disc_block(feature_maps * 2, feature_maps * 4),\r\n","            self._make_disc_block(feature_maps * 4, feature_maps * 8),\r\n","            self._make_disc_block(feature_maps * 8, 1, kernel_size=4, stride=1, padding=0, last_block=True),\r\n","        )\r\n","\r\n","    @staticmethod\r\n","    def _make_disc_block(\r\n","        in_channels: int,\r\n","        out_channels: int,\r\n","        kernel_size: int = 4,\r\n","        stride: int = 2,\r\n","        padding: int = 1,\r\n","        bias: bool = False,\r\n","        batch_norm: bool = True,\r\n","        last_block: bool = False,\r\n","    ) -> nn.Sequential:\r\n","        if not last_block:\r\n","            disc_block = nn.Sequential(\r\n","                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\r\n","                nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\r\n","                nn.LeakyReLU(0.2, inplace=True),\r\n","            )\r\n","        else:\r\n","            disc_block = nn.Sequential(\r\n","                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\r\n","                nn.Sigmoid(),\r\n","            )\r\n","\r\n","        return disc_block\r\n","\r\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n","        return self.disc(x).view(-1, 1).squeeze(1)\r\n","\r\n","class DCGAN(pl.LightningModule):\r\n","    \"\"\"DCGAN implementation.\r\n","    Example::\r\n","        from pl_bolts.models.gans import DCGAN\r\n","        m = DCGAN()\r\n","        Trainer(gpus=2).fit(m)\r\n","    Example CLI::\r\n","        # mnist\r\n","        python dcgan_module.py --gpus 1\r\n","        # cifar10\r\n","        python dcgan_module.py --gpus 1 --dataset cifar10 --image_channels 3\r\n","    \"\"\"\r\n","\r\n","    def __init__(\r\n","        self,\r\n","        beta1: float = 0.5,\r\n","        feature_maps_gen: int = 64,\r\n","        feature_maps_disc: int = 64,\r\n","        image_channels: int = 1,\r\n","        latent_dim: int = 100,\r\n","        learning_rate: float = 0.0002,\r\n","        **kwargs: Any,\r\n","    ) -> None:\r\n","        \"\"\"\r\n","        Args:\r\n","            beta1: Beta1 value for Adam optimizer\r\n","            feature_maps_gen: Number of feature maps to use for the generator\r\n","            feature_maps_disc: Number of feature maps to use for the discriminator\r\n","            image_channels: Number of channels of the images from the dataset\r\n","            latent_dim: Dimension of the latent space\r\n","            learning_rate: Learning rate\r\n","        \"\"\"\r\n","        super().__init__()\r\n","        self.save_hyperparameters()\r\n","\r\n","        self.generator = self._get_generator()\r\n","        self.discriminator = self._get_discriminator()\r\n","\r\n","        self.criterion = nn.BCELoss()\r\n","\r\n","    def _get_generator(self) -> nn.Module:\r\n","        generator = DCGANGenerator(self.hparams.latent_dim, self.hparams.feature_maps_gen, self.hparams.image_channels)\r\n","        generator.apply(self._weights_init)\r\n","        return generator\r\n","\r\n","    def _get_discriminator(self) -> nn.Module:\r\n","        discriminator = DCGANDiscriminator(self.hparams.feature_maps_disc, self.hparams.image_channels)\r\n","        discriminator.apply(self._weights_init)\r\n","        return discriminator\r\n","\r\n","    @staticmethod\r\n","    def _weights_init(m):\r\n","        classname = m.__class__.__name__\r\n","        if classname.find(\"Conv\") != -1:\r\n","            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n","        elif classname.find(\"BatchNorm\") != -1:\r\n","            torch.nn.init.normal_(m.weight, 1.0, 0.02)\r\n","            torch.nn.init.zeros_(m.bias)\r\n","\r\n","    def configure_optimizers(self):\r\n","        lr = self.hparams.learning_rate\r\n","        betas = (self.hparams.beta1, 0.999)\r\n","        opt_disc = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=betas)\r\n","        opt_gen = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=betas)\r\n","        return [opt_disc, opt_gen], []\r\n","\r\n","    def forward(self, noise: torch.Tensor) -> torch.Tensor:\r\n","        \"\"\"Generates an image given input noise.\r\n","        Example::\r\n","            noise = torch.rand(batch_size, latent_dim)\r\n","            gan = GAN.load_from_checkpoint(PATH)\r\n","            img = gan(noise)\r\n","        \"\"\"\r\n","        noise = noise.view(*noise.shape, 1, 1)\r\n","        return self.generator(noise)\r\n","\r\n","    def training_step(self, batch, batch_idx, optimizer_idx):\r\n","        real, _ = batch\r\n","\r\n","        # Train discriminator\r\n","        result = None\r\n","        if optimizer_idx == 0:\r\n","            result = self._disc_step(real)\r\n","\r\n","        # Train generator\r\n","        if optimizer_idx == 1:\r\n","            result = self._gen_step(real)\r\n","\r\n","        return result\r\n","\r\n","    def _disc_step(self, real: torch.Tensor) -> torch.Tensor:\r\n","        disc_loss = self._get_disc_loss(real)\r\n","        self.log(\"loss/disc\", disc_loss, on_epoch=True)\r\n","        return disc_loss\r\n","\r\n","    def _gen_step(self, real: torch.Tensor) -> torch.Tensor:\r\n","        gen_loss = self._get_gen_loss(real)\r\n","        self.log(\"loss/gen\", gen_loss, on_epoch=True)\r\n","        return gen_loss\r\n","\r\n","    def _get_disc_loss(self, real: torch.Tensor) -> torch.Tensor:\r\n","        # Train with real\r\n","        real_pred = self.discriminator(real)\r\n","        real_gt = torch.ones_like(real_pred)\r\n","        real_loss = self.criterion(real_pred, real_gt)\r\n","\r\n","        # Train with fake\r\n","        fake_pred = self._get_fake_pred(real)\r\n","        fake_gt = torch.zeros_like(fake_pred)\r\n","        fake_loss = self.criterion(fake_pred, fake_gt)\r\n","\r\n","        disc_loss = real_loss + fake_loss\r\n","\r\n","        return disc_loss\r\n","\r\n","    def _get_gen_loss(self, real: torch.Tensor) -> torch.Tensor:\r\n","        # Train with fake\r\n","        fake_pred = self._get_fake_pred(real)\r\n","        fake_gt = torch.ones_like(fake_pred)\r\n","        gen_loss = self.criterion(fake_pred, fake_gt)\r\n","\r\n","        return gen_loss\r\n","\r\n","    def _get_fake_pred(self, real: torch.Tensor) -> torch.Tensor:\r\n","        batch_size = len(real)\r\n","        noise = self._get_noise(batch_size, self.hparams.latent_dim)\r\n","        fake = self(noise)\r\n","        fake_pred = self.discriminator(fake)\r\n","\r\n","        return fake_pred\r\n","\r\n","    def _get_noise(self, n_samples: int, latent_dim: int) -> torch.Tensor:\r\n","        return torch.randn(n_samples, latent_dim, device=self.device)\r\n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["flower_dm = ImageDataModule(config.data_dir, config.min_image_size,\r\n","                            config.image_size, config.batch_size)\r\n","\r\n","model = DCGAN(image_channels=config.n_channels)\r\n","\r\n","trainer = pl.Trainer(\r\n","    gpus=1 if config.gpu else 0,\r\n","    max_epochs=config.n_epochs,\r\n",")"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["trainer.fit(model, datamodule=flower_dm)\r\n","\r\n","demo_single(model)\r\n","demo_grid(model)"],"outputs":[],"metadata":{"trusted":true}}]}